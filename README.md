# ğŸ“˜ Guide de configuration et dâ€™exÃ©cution du projet *Fake News Detect*

## âš™ï¸ 0. Installation des dÃ©pendances

Il faut crÃ©er l'environnement 
```bash
python3 -m venv .venv
source .venv/bin/activate
```

Avant de commencer, assurez-vous dâ€™avoir installÃ© et synchronisÃ© toutes les dÃ©pendances du projet Ã  lâ€™aide de **uv** :

```bash
uv sync
```

TÃ©lÃ©charger le modÃ¨le

```bash
python -m spacy download en_core_web_sm
```

## 1. Chargement des donnÃ©es

Utilisez le lien suivant pour tÃ©lÃ©charger le jeu de donnÃ©es :  
ğŸ‘‰ [Fake and Real News Dataset â€“ Kaggle](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset/data)

AprÃ¨s le tÃ©lÃ©chargement, placez les fichiers dans le rÃ©pertoire racine du projet :
___

```bash
/Fake-News-Detect/
â”‚
â”œâ”€â”€ True.csv
â””â”€â”€ Fake.csv
```

CrÃ©ez ensuite une structure de dossiers pour organiser vos donnÃ©es :

___

```bash
/Fake-News-Detect/
â”‚
â””â”€â”€ data/
â”œâ”€â”€ raw/
â””â”€â”€ processed/
```

Pour extraire un nombre spÃ©cifique de lignes dâ€™un fichier CSV (par exemple, pour un test rapide), utilisez la commande suivante :

```bash
head -n <n_lignes> input.csv > data/raw/output.csv
```
Remplacez <n_lignes> par le nombre de lignes souhaitÃ©.

## 2. ExÃ©cution du pipeline de traitement
Lancez le script suivant pour traiter vos deux fichiers CSV (True.csv et Fake.csv) :

```bash
python process_data/pipeline.py
```

Une fois le traitement terminÃ©, un fichier CSV final sera gÃ©nÃ©rÃ© dans le dossier :

```bash
data/processed/
```

Ce fichier fusionnera les deux sources et contiendra les colonnes suivantes :

| Colonnes         | Description                              |
| ---------------- | -----------------------------------------|
| article_id       | Identifiant unique de lâ€™article          |
| chunk_id         | Identifiant du segment (chunk) de texte  |
| text             | Contenu textuel de lâ€™article             |
| label            | Classe de lâ€™article : Fake ou True       |

## 3. CrÃ©ation de la base de donnÃ©es Chroma
Tout se fait dÃ©sormais en une seule commande via le pipeline principal :
```bash
python database/chroma_pipeline.py
```
Ce script :

  - lit automatiquement le fichier data/processed/chunks.csv,

  - crÃ©e la collection fake_news_collection (si elle nâ€™existe pas),

  - gÃ©nÃ¨re les embeddings pour chaque chunk,

  - insÃ¨re les donnÃ©es dans la base par lots (batch_size=50 par dÃ©faut).

AprÃ¨s lâ€™exÃ©cution, vÃ©rifiez le contenu de la base de donnÃ©es avec :
```bash
python database/check_db.py
```
Ce script vous permettra de confirmer le nombre de chunks prÃ©sents dans la collection.

## 4. Utilisation de lâ€™interface Streamlit
Une fois la base configurÃ©e, vous pouvez lancer lâ€™interface utilisateur avec Streamlit :
```bash
streamlit run interface/app.py
```

Cette interface vous permettra dâ€™entrer un texte et dâ€™obtenir une prÃ©diction au format suivant :
```bash
RÃ©sultat : Fake ou True
Explication : Raisonnement du modÃ¨le
```
## Structure de projet

```bash
â”œâ”€â”€ README.md
â”œâ”€â”€ chroma_db
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ processed
â”‚   â”‚   â””â”€â”€ chunks.csv
â”‚   â””â”€â”€ raw
â”‚       â”œâ”€â”€ Fake.csv
â”‚       â””â”€â”€ True.csv
â”œâ”€â”€ database
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ check_db.py
â”‚   â”œâ”€â”€ chroma_insert.py
â”‚   â”œâ”€â”€ chroma_pipeline.py
â”‚   â”œâ”€â”€ chroma_setup.py
â”‚   â””â”€â”€ chroma_utils.py
â”œâ”€â”€ interface
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ exploration.ipynb
â”œâ”€â”€ process_data 
â”‚   â”œâ”€â”€ article_processor.py
â”‚   â”œâ”€â”€ base_article.py
â”‚   â”œâ”€â”€ chunked_article.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â””â”€â”€ preprocessing_article.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ rag_system
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chroma_retrieval.py
â”‚   â”œâ”€â”€ ollama_generation.py
â”‚   â”œâ”€â”€ query_preprocess.py
â”‚   â””â”€â”€ rag_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ uv.lock
```
